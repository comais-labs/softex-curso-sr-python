{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paralelismo Distribuído"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Paralelismo Distribuído refere-se à execução de tarefas em paralelo em várias máquinas em uma rede. No contexto de MLOps e aprendizado de máquina, isso geralmente se refere ao treinamento ou inferência distribuída de modelos em vários nós ou máquinas.\n",
    "\n",
    "Uma das bibliotecas mais populares para paralelismo distribuído em Python é o `Dask`. O Dask permite que operações como treinamento de modelo e processamento de dados sejam distribuídas em vários nós de um cluster.\n",
    "\n",
    "Vamos criar um exemplo simples usando o Dask e o Scikit-learn para demonstrar o paralelismo distribuído.\n",
    "\n",
    "### Para instalar dask:\n",
    "\n",
    "- via conda:\n",
    "    - conda install dask\n",
    "    - conda install -c conda-forge dask-ml\n",
    "- via pip:\n",
    "    - pip install dask\n",
    "    - pip install dask-ml\n",
    "\n",
    "Neste exemplo:\n",
    "\n",
    "1. Criamos um cluster Dask local com 4 workers.\n",
    "2. Geramos um conjunto de dados distribuído usando `dask_ml.datasets.make_classification`.\n",
    "3. Dividimos os dados em conjuntos de treinamento e teste.\n",
    "4. Treinamos um modelo de floresta aleatória usando Scikit-learn. Embora o modelo seja treinado localmente, o Dask gerencia a distribuição dos dados.\n",
    "5. Calculamos a precisão do modelo.\n",
    "\n",
    "Este é um exemplo simples de paralelismo distribuído em uma única máquina usando o Dask. Em um cenário real, você pode configurar um cluster Dask distribuído em várias máquinas para escalar ainda mais o processamento.\n",
    "\n",
    "Para verdadeiro treinamento distribuído em vários nós, você pode explorar bibliotecas como Horovod ou os recursos de treinamento distribuído do PyTorch e TensorFlow. Estas bibliotecas permitem treinar modelos de aprendizado profundo em vários nós e GPUs em paralelo.\n",
    "\n",
    "### Exemplo: Treinamento Distribuído com Dask e Scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_ml.datasets import make_classification\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from dask_ml.metrics import accuracy_score\n",
    "\n",
    "# Criar um cluster local e um cliente para interagir com esse cluster\n",
    "cluster = LocalCluster(n_workers=4)\n",
    "client = Client(cluster)\n",
    "\n",
    "# Gerar um conjunto de dados distribuído\n",
    "X, y = make_classification(n_samples=100000, n_features=20, chunks=10000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Treinar um modelo de floresta aleatória distribuído\n",
    "clf = RandomForestClassifier()\n",
    "clf = clf.fit(X_train.compute(), y_train.compute())  # .compute() traz os dados para a memória local\n",
    "\n",
    "# Prever e calcular a precisão\n",
    "y_pred = clf.predict(X_test.compute())\n",
    "accuracy = accuracy_score(y_test.compute(), y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Fechar o cliente e o cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
