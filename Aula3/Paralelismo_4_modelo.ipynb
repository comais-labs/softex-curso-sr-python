{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paralelismo a Nível de Modelo (Model Parallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Paralelismo a Nível de Modelo, ou \"Model Parallelism\", refere-se à técnica de dividir um modelo de aprendizado profundo em partes e executar cada parte em um dispositivo diferente, geralmente GPUs. Isso é particularmente útil quando o modelo é muito grande para caber em uma única GPU.\n",
    "\n",
    "Vamos criar um exemplo simples usando o PyTorch para demonstrar o paralelismo a nível de modelo. Neste exemplo, vamos dividir um modelo de rede neural simples em duas partes e executar cada parte em uma GPU diferente.\n",
    "\n",
    "### Exemplo: Model Parallelism com PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Verificar a disponibilidade de GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "# Modelo de rede neural dividido em duas partes\n",
    "class SplitNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SplitNN, self).__init__()\n",
    "        self.part1 = nn.Sequential(\n",
    "            nn.Linear(10, 50),\n",
    "            nn.ReLU()\n",
    "        ).cuda(0)  # Primeira parte do modelo na GPU 0\n",
    "        \n",
    "        self.part2 = nn.Sequential(\n",
    "            nn.Linear(50, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 1)\n",
    "        ).cuda(1)  # Segunda parte do modelo na GPU 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.part1(x.cuda(0))\n",
    "        return self.part2(x.cuda(1))\n",
    "\n",
    "# Criando dados sintéticos\n",
    "X = torch.rand(100, 10)\n",
    "\n",
    "# Treinamento\n",
    "model = SplitNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, torch.ones(100, 1).cuda(1)) \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/100], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo:\n",
    "\n",
    "1. Criamos um modelo `SplitNN` que é dividido em duas partes. A primeira parte é colocada na GPU 0 e a segunda parte na GPU 1.\n",
    "2. Durante a passagem para a frente (`forward`), os dados são passados pela primeira parte do modelo na GPU 0, depois são transferidos para a GPU 1 e passados pela segunda parte do modelo.\n",
    "3. O treinamento é realizado normalmente, mas os gradientes são calculados e propagados de volta através de ambas as partes do modelo em suas respectivas GPUs.\n",
    "\n",
    "Este código assume que você tem pelo menos duas GPUs disponíveis. Se você tiver apenas uma GPU ou estiver executando em uma máquina sem GPUs, precisará ajustar o código de acordo.\n",
    "\n",
    "O paralelismo a nível de modelo é uma técnica avançada e pode ser complicado de implementar em modelos mais complexos. No entanto, pode ser uma solução valiosa quando se trabalha com modelos muito grandes que não cabem em uma única GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
